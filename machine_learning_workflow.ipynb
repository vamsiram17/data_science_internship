{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accompanied-defeat",
   "metadata": {},
   "source": [
    "# Workflow of a Machine Learning project\n",
    "\n",
    "**Name : Vamsi Ram**\n",
    "\n",
    "**E-mail : vamsiramg@gmail.com**\n",
    "\n",
    "This Jupyter notebook includes the summary of my learnings from the resources provided in the Data Science Internship by wowlabz on conduira online platform.\n",
    "\n",
    "Raw data can be of any form and we need to ensure that the data should be modelled perfectly before fitting it into a machine learning model.I would like to explain how a machine learning project is categorized with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-delicious",
   "metadata": {},
   "source": [
    "Considering the case of an e-commerce website which in this tutorial is Amazon,a machine learning problem in this case is broadly classified into different categories as,\n",
    "\n",
    "- Ranking – Helping users to find the most relevant things.\n",
    "- Recommendation – Giving users with the things they are most interested in.\n",
    "- Classification – Figuring out the what kind of a category.\n",
    "- Regression – Predicting a numerical value of a particular thing.\n",
    "- Clustering – Grouping similar things together.\n",
    "- Anomaly Detection – Finding uncommon things among a group.\n",
    "\n",
    "If the data is provided with correct labels, it is **Supervised Learning**.\n",
    "If the data is provided without labels, it is **Unsupervised Learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-legislature",
   "metadata": {},
   "source": [
    "## Exploratory data analysis:\n",
    "\n",
    "In general, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
    "\n",
    "There might be some cases in which there would be imbalance between the labels of the data.This imbalance is **Class Imbalance.\n",
    "\n",
    "This can be rectified by certain methods such as,\n",
    "\n",
    "- Down-Sampling – reducing the size of frequent classes\n",
    "- Up-Sampling – increase the size of the smaller classes\n",
    "- Data Generation – create similar new records\n",
    "- Sample weights – assign higher weights to frequent classes and lower weights to smaller classes.\n",
    "\n",
    "In some cases,we need to add or drop certain labels accordingly.Imputing or filling the missing values with the label mean or median or mode\n",
    "\n",
    "~~~\n",
    "data[‘column_name’].fillna((data[‘column_name’].mean()) - to replace with mean\n",
    "data[‘column_name’].fillna((data[‘column_name’].mean()) - to replace with median\n",
    "data[‘column_name’].fillna((data[‘column_name’].mean()) - to replace with mode\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-korean",
   "metadata": {},
   "source": [
    "## Feature Engineering:\n",
    "\n",
    "It is a process of creating novel features as inputs for the machine learning models from the raw data provided, by using the domain and data knowledge.\n",
    "\n",
    "This can be done by encoding the Categorical features which can performed by,\n",
    "\n",
    "- Using **LabelEncoder** in sklearn which encodes one feature with **.fit() and .transform()** methods.\n",
    "- Using **OrdinalEncoder** to encode two or more categorical features with .fit() and .transform() methods.\n",
    "- Target encoding, which encodes the values that can explain the target.\n",
    "\n",
    "\n",
    "Machine Learning models require well defined numerical data. In case of implementing ML models with text data, we need to perform **Text Vectorization.**\n",
    "\n",
    "**Bag of Words(BoW) method:**\n",
    "\n",
    "It converts text data into numerical features.\n",
    "This can be considered as feature extraction, as we extract the important information from the text and convert it into numeric form.\n",
    "For each word in the text, we get either binary 0 or 1 or it can be the count or frequency of the particular words. \n",
    "\n",
    "**Term Frequency(TF)**: It increase the weight of the common words in a document.\n",
    "\n",
    "Bag of Words method can be implemented using the CountVectorizer method in the sklearn. It converts a collection of text documents to a matrix of token counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-absence",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Appropriate machine learning model needs to be selected in order to obtain better prediction results.With the help of Exploratory Data Analysis and Feature Enginerring we would obtain the type of data we are dealing with and then we can select a suitable machine learning algorithm.\n",
    "\n",
    "Some of the machine leraning algorithms are:\n",
    "\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Tree based\n",
    "- Neural Netwroks\n",
    "- KNN\n",
    "- SVM\n",
    "\n",
    "\n",
    "Linear Regression: This can be used for supervised machine learning problems where the target label is a continuous value.\n",
    "\n",
    "Logistic Regression: This can be used when the target label is a binary dependent variable (classification problems).\n",
    "\n",
    "SVM: This is a supervised machine learning algorithm which can be used for classification problems.\n",
    "\n",
    "Tree based and Neural Networks would be suitable for both Supervised and Unsupervised data.\n",
    "\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "\n",
    "It predicts the new data points based on the K similar records from the dataset.\n",
    "\n",
    "It looks at the K closest data points, where K is initially specified\n",
    "Calculates the distances from a centroid to all the other data points.\n",
    "It finds the K nearest neighbors.\n",
    "\n",
    "This process is continuously repeated, optimizing the centroid so that majority of the data points are classified.\n",
    "\n",
    "**Feature Scaling:**\n",
    "\n",
    "Feature should be on the same scale when using KNN.We can bring features in the data to a similar scale by:\n",
    "\n",
    "- Mean/Variance standardization\n",
    "- MinMax Scaling\n",
    "\n",
    "**StandardScaler** in **sklearn scaler** will scale the values to be centered around with mean = 0 and standard deviation 1. \n",
    "\n",
    "**MinMax Scaler** will scale the values so that the minimum value is 0 and maximum value is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-coating",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "\n",
    "After defining and training a model, we can make use of performance metrics in order to see how our model is performing,\n",
    "\n",
    "In case of **Regression metrics** there are\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- R Squared\n",
    "\n",
    "In case of **Classification metrics**,\n",
    "\n",
    "- True Positive(TP) – Predicted positive when the actual is positive.\n",
    "- False Positive (FP)– Predicted positive when the actual is negative.\n",
    "- False Negative (FN)– Predicted negative when the actual is positive\n",
    "- True Negative (TN)– Predicted negative when the actual is negative.\n",
    "\n",
    "These four parameters can be visualized in the form of a **Confusion Matrix.**\n",
    "With the help of the above obtained parameters, we can obtain certain performance metrics of the classification model.\n",
    "\n",
    "**Accuracy** = TP+TN/(TP+FP+FN+TN)\n",
    "\n",
    "**Recall**: Out of true values how many were we able to predict.\n",
    "\n",
    "  Recall = TP/(TP+FN) .Always the recall should be more\n",
    "\n",
    "**Precision** = TP/(TP+FP)\n",
    "\n",
    "F1 score, a combined metric which is the harmonic mean of Precision and Recall.\n",
    "\n",
    "\n",
    "**K-fold Cross Validation:**\n",
    "\n",
    "- It is a validation technique to see how well a trained model generalizes to an independent validation set.\n",
    "- It sets aside Kth fold of the data for validation and trains the model on the remaining training dataset.\n",
    "- Then it tests the model on the validation set and averages the model performance metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-dover",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hence in this notebook I have briefly explained the workflow of a machine learning project.From data collection, to exploratory data analysis followed by feature engineering,model selection and model evaluation, each and every step in the machine learning workflow should be carried out with precision so that we can obtain a better predicitng model.\n",
    "\n",
    "I am greatly obliged and thank wowlabz for providing me the valuable learning resources in order to kickstart my internship.I also take this oppurtunity to thank Conduira online for providing me a platform in order to learn and develop my skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
