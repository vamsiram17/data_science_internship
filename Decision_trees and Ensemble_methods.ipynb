{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooperative-blackberry",
   "metadata": {},
   "source": [
    "# Decision Trees and Ensemble Methods\n",
    "\n",
    "Name   : Vamsi Ram\n",
    "E-mail : vamsiramg@gmail.com\n",
    "\n",
    "This Jupyter notebook includes the summary of my learnings from the resources provided in the Data Science Internship by wowlabz on conduira online platform.\n",
    "\n",
    "## Tree based: \n",
    "These are simple set of machine learning algorithms that allows to make predictions.\n",
    "\n",
    "## Ensembling techniques:\n",
    "These are the ways in which we can combine several machine learning models into a single one in order to make predictions.\n",
    "\n",
    "Advantages of the ensembled tree based methods are:\n",
    "\n",
    "- They are **faster**, because many of them are not based on gradient optimization\n",
    "- With ensembling, it is easier to **control overfitting**.\n",
    "- These methods are explicit and easier to understand.\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "A decision tree can be defined as series of yes/no questions about the input data which are processed in a sequence.\n",
    "\n",
    "We have to specify the split condition in such a way that we obtain the purest child nodes at the end.\n",
    "After each split if we obtain purely classified set of data points, then we can specify the resulting node as leaf node. Whereas if not, we need to further split.\n",
    "\n",
    "In order to **avoid over fitting** there are certain conditions to be considered by specifying a **stopping criterion** for splitting as to stop,\n",
    "\n",
    "- Once you have reached a maximum depth.\n",
    "- Once you have reached a maximum number of leaves.\n",
    "- Once there are too few data points in a particular leaf.\n",
    "- Once the leaf has met some desired level of purity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-baltimore",
   "metadata": {},
   "source": [
    "### Impurity functions:\n",
    "\n",
    "Before splitting we need to ask a question at the node. While asking this question we need to consider the impurity in its child node (node after splitting).\n",
    "This impurity in the child node should be as low as possible.\n",
    "\n",
    "**Impurities for classification**:\n",
    "\n",
    "- Entropy\n",
    "- Gini\n",
    "The computation of entropy is complex as it involves logarithms, so Gini is preferred to compute and it is taken as by default.\n",
    "\n",
    "**Impurities for regression**:\n",
    "\n",
    "- Variance\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "\n",
    "\n",
    "### Information Gain:\n",
    "\n",
    "It is the difference between the impurity of the parent node and the weighted sum of impurities of the child nodes.\n",
    "Also, the lower the impurity of the child nodes, the larger is the information gain.\n",
    "\n",
    "### CART Algorithm:\n",
    "\n",
    "- It stands for Classification and Regression Trees.\n",
    "- It constructs binary trees which consists of yes/no. \n",
    "\n",
    "The basic methodology of this algorithm is,\n",
    "\n",
    "- It splits the dataset into two parts, as left and right.\n",
    "- To keep track of the split that maximizes the average decrease in impurity.\n",
    "- Recursively pass the left and right datasets to the child nodes.\n",
    "- If the stopping criteria is met, it identifies the value to return.\n",
    "\n",
    "With tree based methods, in order to control the complexity we control the number of decision nodes.\n",
    "These can be controlled directly by limiting the number of leaves, or else by imposing a maximum depth.\n",
    "Pre pruning technique which involves stopping early when some criteria is not met and it works entirely on training data.\n",
    "\n",
    "Basic methodology to implement a classifier based Decision Tree in Python:\n",
    "\n",
    "(Note:This below code is just for illustration purpose and it is my humble request not to run in the kernel as it may lead to error as there are no data files that are exported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Let us assume we are having X and Y where X is the predictor and Y is the target label\n",
    "#for training data set and x_test(predictor) of test_dataset\n",
    "#Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "# also we can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "#for regression\n",
    "model = tree.DecisionTreeRegressor() \n",
    "#Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-indianapolis",
   "metadata": {},
   "source": [
    "There are five important constraints that need to be specified on the tree based algorithms, which are:\n",
    "\n",
    "- Minimum samples of a node split – **min_samples_split**\n",
    "- Minimum samples of a leaf node – **min_samples_leaf**\n",
    "- Maximum depth of the tree  - **max_depth**\n",
    "- Maximum number of terminal nodes \n",
    "- Maximum features to consider for split – **max_features**\n",
    "\n",
    "As we increase the maximum number of leaf nodes and the depth of the tree, the decision tree would become more and more complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-crime",
   "metadata": {},
   "source": [
    "## Bias Variance Tradeoff:\n",
    "\n",
    "The errors in the model can be attributed to mainly a combination of tree types of errors which are:\n",
    "\n",
    "- Systematic prediction error (bias)\n",
    "- Fluctuation of the model with the selected dataset (variance)\n",
    "- Inherent noise.\n",
    "\n",
    "The bias and variance can be controlled through model selection.\n",
    "\n",
    "- Bias: It is the systematic prediction error that come due to the model selection and assumptions\n",
    "- Variance: It measures the variability of the model when it fits over the selected dataset.\n",
    "\n",
    "**Underfitting** occurs due to the low model complexity.\n",
    "This corresponds to high bias because the model does not detect the underlying important features and also due to which it has low variance.\n",
    "\n",
    "**Overfitting** occurs due to the over complex model and it doesn’t generalize.\n",
    "This corresponds to low bias because the model overfits and high variance.\n",
    "\n",
    "Ensemble learning is one of the ways to maintain the balance and trade off between the bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-miami",
   "metadata": {},
   "source": [
    "## Bootstrapping:\n",
    "\n",
    "It is a resampling technique used to estimate certain statistics on a population by sampling the dataset with replacement.\n",
    "With this technique we can create larger dataset by drawing samples with replacement from an original dataset with preserved distribution.\n",
    "\n",
    "The **resample()** scikit-learn function can be utilized. It takes as arguments the data array, whether to sample or not to sample with replacement, the size of the sample, and the seed for the pseudorandom number generator used prior to the sampling.\n",
    "\n",
    "we can create a bootstrap that creates a sample with replacement with 8 observations and uses a value of 1 for the pseudorandom number generator.\n",
    "\n",
    "\n",
    "**boot = resample(data, replace=True, n_samples=8, random_state=1)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-gabriel",
   "metadata": {},
   "source": [
    "## Bagging:\n",
    "\n",
    "Bagging is an ensemble technique which can be used to reduce the variance of the predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set.\n",
    "\n",
    "This involves three main steps:\n",
    "\n",
    "- Creating multiple datasets:\n",
    "- It performs sampling with replacement on the original data such that new datasets are formed.\n",
    "- The new data sets could have a fraction of the rows and columns, which are generally considered as hyper-parameters in a bagging model.Taking row and column fractions less than 1 will help in making robust models, less prone to overfitting.\n",
    "\n",
    "**Build multiple classifiers**:\n",
    "Classifiers are built individually on each of these datasets.\n",
    "In general the same type of classifier is built and modelled on each of these datasets and predictions are made.\n",
    "\n",
    "**Combine classifiers**:\n",
    "The predictions made by all of these classifiers are combined by using any of the  statistical metric such as mean, median or mode.This combined classifier is more robust when compared to a single model.\n",
    "\n",
    "**BaggingClassifier(base_estimator=None,n_estimators=10,max_samples=1.0, bootstrap=True,oob_score=False)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-participation",
   "metadata": {},
   "source": [
    "## Random Forest:\n",
    "\n",
    "- It is an ensemble learning method where in a group of weak models combine together to form a powerful model.\n",
    "- This algorithm can be used to solve both type of problems i.e. classification and regression.\n",
    "- It can handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods.\n",
    "\n",
    "The model outputs Importance of variable, which can be a very handy feature on some random data set.\n",
    "\n",
    "**importance=model.feature_importances_**\n",
    "\n",
    "The property of dimensionality reduction will be helpful for the PCA analysis,\n",
    "Principal Component Analysis in various projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-democracy",
   "metadata": {},
   "source": [
    "## Boosting:\n",
    "\n",
    "This is a technique which converts weak learning models into strong learning models.In order to convert weak learner to strong learner, we combine the prediction of each weak learner using methods like:\n",
    "\n",
    "- Using average/ weighted average\n",
    "- Considering prediction has higher vote\n",
    "\n",
    "**Gradient Boosting:** Gradient boosting mainly involves three elements:\n",
    "\n",
    "- To optimize a loss function.\n",
    "- Initially using a weak learner to make predictions.\n",
    "- An additive model to add weak learners to minimize the loss function.\n",
    "\n",
    "**XG Boost:**\n",
    "\n",
    "- It means extreme gradient boosting.\n",
    "- In order to trying all possible split thresholds, it is cumbersome for large datasets,\n",
    "- XG boost uses quantiles as the threshold conditions for splitting.\n",
    "\n",
    "It generally makes use of three quantiles 0.25,0.5 and 0.75 as the splits to try.\n",
    "Some of the default parameters in this interface include max_depth, learning rate,n_estimators and booster.\n",
    "\n",
    "**Light GBM:**\n",
    "\n",
    "It focusses more on the under-trained data points while building the trees by using gradients as a measure of importance.\n",
    "This feature is called as **Gradient Based One Side Sampling (GOSS)**.\n",
    "It sorts the data according to the absolute gradient values and perform sampling and then arrange according to the importance.\n",
    "\n",
    "It also uses **Exclusive Feature Bundling (EFB)**, which merges many features that are never non-zero. This is advantageous when dealing with large datasets.\n",
    "\n",
    "CatBoost mainly focusses on processing categorical features and boosting trees based on an ordering principle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-nicholas",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The tree based algorithms and ensembling methods are important to learn in the data science domain.The tree models are known to provide the best model performance in the family of the machine learning algorithms.\n",
    "\n",
    "I have given brief description of **decison trees**, the functions and methods to determine the model performance fo the decision trees with trade-off between bias and variance. Different techniques such as **bootstrapping, bagging, Random Forest and Boosting**, which are used to improve the model performance are also discussed.\n",
    "\n",
    "I am greatly obliged and thank wowlabz for providing me the valuable learning resources in order to kickstart my internship.I also take this oppurtunity to thank Conduira online for providing me a platform in order to learn and develop my skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
